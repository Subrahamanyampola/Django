{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM3MlNEquXeqHWRrVzUYBld",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Subrahamanyampola/Django/blob/master/Chrun%20Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Introduction\n",
        "\n",
        "In the telecommunications industry, customer churn prediction is crucial for reducing customer turnover. By using predictive analytics and data mining techniques, telecom providers can proactively identify customers likely to churn, enabling them to implement retention strategies. This project aims to develop predictive models for customer churn using the IBM Telco Customer Churn dataset.\n",
        "\n",
        "The machine learning models used for this project are:\n",
        "- Decision Trees\n",
        "- Logistic Regression\n",
        "- Support Vector Machines (SVM)\n"
      ],
      "metadata": {
        "id": "DYkfx4DEMO6G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Data Collection & Preprocessing\n",
        "\n",
        "In this section, we will load the dataset, handle missing values, encode categorical variables, normalize data, and address any class imbalance issues.\n",
        "\n",
        "## 2.1 Load Dataset\n"
      ],
      "metadata": {
        "id": "g_UXTxoSMUiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "metadata": {
        "id": "RePVtMqsMVWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Handle Missing Values\n"
      ],
      "metadata": {
        "id": "4NrnkUGTMcQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "df.isnull().sum()\n",
        "\n",
        "# Handle missing values (e.g., filling with median, mode, or dropping rows)\n",
        "df.fillna(df.median(), inplace=True)  # Example for filling numeric columns with median\n"
      ],
      "metadata": {
        "id": "9w-_fa_0Me1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Encode Categorical Variables\n"
      ],
      "metadata": {
        "id": "AAuuayF4MhRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of encoding categorical variables\n",
        "df = pd.get_dummies(df, drop_first=True)  # One-hot encoding for categorical columns\n"
      ],
      "metadata": {
        "id": "_9RF5J-tMjoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Normalize Data\n"
      ],
      "metadata": {
        "id": "z-dFH-QbMl9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Normalizing the data\n",
        "scaler = StandardScaler()\n",
        "df_scaled = scaler.fit_transform(df.drop('Churn', axis=1))  # Exclude the target column\n"
      ],
      "metadata": {
        "id": "-EWN62wIMn9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 Address Class Imbalance\n"
      ],
      "metadata": {
        "id": "bMU-lo_lMqce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Apply SMOTE to balance the classes\n",
        "smote = SMOTE()\n",
        "X_res, y_res = smote.fit_resample(df_scaled, df['Churn'])\n"
      ],
      "metadata": {
        "id": "Tub8zwaYMsYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Exploratory Data Analysis (EDA) & Feature Engineering\n",
        "\n",
        "In this section, we will explore the dataset, visualize important patterns, and select relevant features.\n",
        "\n",
        "## 3.1 Visualize Churn Distribution\n"
      ],
      "metadata": {
        "id": "4BE0t24UMvWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Plot churn distribution\n",
        "sns.countplot(x='Churn', data=df)\n",
        "plt.title('Churn Distribution')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KcGpG2WrMygO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Correlation Matrix\n"
      ],
      "metadata": {
        "id": "EkBoRZLRM3rd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation matrix to find important features\n",
        "corr_matrix = df.corr()\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8n4dBhzsM5ti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Feature Selection\n"
      ],
      "metadata": {
        "id": "7o6P5u6TM7x_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Feature selection using Recursive Feature Elimination (RFE)\n",
        "model = LogisticRegression()\n",
        "selector = RFE(model, n_features_to_select=10)\n",
        "selector = selector.fit(df_scaled, df['Churn'])\n",
        "selected_features = df.columns[selector.support_]\n",
        "print(\"Selected Features:\", selected_features)\n"
      ],
      "metadata": {
        "id": "j7gDaBdvM9lO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Modeling & Evaluation\n",
        "\n",
        "In this section, we will train the models using Decision Trees, Logistic Regression, and Support Vector Machines (SVM) and evaluate their performance using K-Fold cross-validation.\n",
        "\n",
        "## 4.1 Model Initialization\n"
      ],
      "metadata": {
        "id": "BFGUrQ4mNAmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize models\n",
        "models = {\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'SVM': SVC()\n",
        "}\n"
      ],
      "metadata": {
        "id": "18mwJccsNC1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Model Training & Cross-Validation\n"
      ],
      "metadata": {
        "id": "IB07ll-ZNE2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate each model using cross-validation\n",
        "for model_name, model in models.items():\n",
        "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
        "    print(f\"{model_name} Accuracy: {scores.mean():.4f}\")\n"
      ],
      "metadata": {
        "id": "wUn_h1mCNG9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Model Evaluation Metrics\n"
      ],
      "metadata": {
        "id": "BdiyJ1ipNJA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Choose the best model (e.g., SVM) for evaluation\n",
        "best_model = SVC()\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate the model\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "7bu6OwQiNKvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Visualization Dashboard\n",
        "\n",
        "In this section, we will build a simple interactive dashboard to visualize the churn probabilities and important features contributing to customer churn. We will use **Plotly** and **Dash**.\n",
        "\n",
        "## 5.1 Build Dashboard (Optional)\n"
      ],
      "metadata": {
        "id": "u0hsAcmfON4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example code for a simple Plotly dashboard (optional, based on project scope)\n",
        "import plotly.express as px\n",
        "\n",
        "# Create a visualization for feature importance (example)\n",
        "fig = px.bar(x=selected_features, y=best_model.coef_[0])\n",
        "fig.update_layout(title=\"Feature Importance\", xaxis_title=\"Features\", yaxis_title=\"Importance\")\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "pn7ACTBmOOwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Conclusion\n",
        "\n",
        "This project aimed to build predictive models for customer churn in the telecommunications industry. By using data mining techniques, we successfully identified key predictors of churn and developed a classification model to predict customer attrition.\n",
        "\n",
        "Future work could involve exploring deep learning techniques or improving the dashboard for better business insights.\n"
      ],
      "metadata": {
        "id": "hBTw5rGDOQ3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OoukSAJiOTB0"
      }
    }
  ]
}